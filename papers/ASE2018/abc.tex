\documentclass[10pt,conference]{IEEEtran}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}

\usepackage{graphicx}

\usepackage{textcomp}
\usepackage{xspace}
\usepackage{booktabs}



\usepackage[table]{xcolor}
\usepackage{paralist}
\usepackage{url}

\usepackage{boxedminipage}
\usepackage{enumitem}
\usepackage{graphicx}  
\graphicspath{
	{figures/}
}
\DeclareGraphicsExtensions{.pdf,.jpeg,.png}


% Complex \xxx for making notes of things to do.  Use \xxx{...} for general
% notes, and \xxx[who]{...} if you want to blame someone in particular.
% Puts text in brackets and in bold font, and normally adds a marginpar
% with the text ``xxx'' so that it is easy to find.  On the other hand, if
% the comment is in a minipage, figure, or caption, the xxx goes in the text,
% because marginpars are not possible in these situations.
{\makeatletter
 \gdef\xxxmark{%
   \expandafter\ifx\csname @mpargs\endcsname\relax % in minipage?
     \expandafter\ifx\csname @captype\endcsname\relax % in figure/caption?
       \marginpar{\textcolor{red}{xxx~}}% not in a caption or minipage, can use marginpar
     \else
       \textcolor{red}{xxx~}% notice trailing space
     \fi
   \else
     \textcolor{red}{xxx~}% notice trailing space
   \fi}
 \gdef\xxx{\@ifnextchar[\xxx@lab\xxx@nolab}
 \long\gdef\xxx@lab[#1]#2{{\bf [\xxxmark \textcolor{red}{#2} ---{\sc #1}]}}
 \long\gdef\xxx@nolab#1{{\bf [\xxxmark \textcolor{red}{#1}]}}
 % This turns them off:
% \long\gdef\xxx@lab[#1]#2{}\long\gdef\xxx@nolab#1{}%
}

\newcommand{\abc}{\textsf{ABC}\xspace}
\title{ABC: Action-Based Test Carving}

\author{\IEEEauthorblockN{Alessio Gambi}
\IEEEauthorblockA{\textit{University of Passau} \\
%\textit{name of organization (of Aff.)}\\
Passau, Germany \\
alessio.gambi@uni-passau.de}%
}


\begin{document}

\maketitle

\begin{abstract}
\end{abstract}

\section{Intro and motivation}
Importance of tests, classes of tests and their pro/cons.
In this paper we focus on system and unit tests, specifically in their use for regression.

System tests are wide scoped, but "real" (~\cite{exysts}).
However, they take long time to complete because they need to setup/deploy the real system, and interact with it.
A common setup is the one to start the SUT and one or more "clients" which interacts with it.
System tests can be automatically generated, for example, via fuzzying and random exploration, assuming a proper driver/infrastructure is available.
For example, this is commonly the case for GUI (Exysts),  Web Application (with crawlers and Selenium) and Apps (Monkey).
%
In general, however, system tests for regression are setup manually: a designer/testers creates them to implement 
specific scenarios and test specific features (~\cite{TDD}-controlla nel libro).
%
For example, login into a system and buy a product, or rent a room. (Maybe better the latter so we keep around this for the test subject). 
%
This "ensures" to capture, during development, refactoring, etc. changes to the code which breaks funcitonalities. 

At the opposite spectrum, there's unit tests. Unit tests are focused, can be easily generated automatically (Evosutie and Randoop), and can achieve high coverage. However, (automatically) generated unit tests they might be "detached" from the reality, that is, they might implement executions which cannot be observed in real life (cite out paper at AST). Units test might turn out to  "waste" precious testing time.
Despite they might capture dangerous or problematic behavior of the SUT (e.g., an uncaught NPE), those problems might be only potential, and it might hard for developers to understand if the failing of a unit test is real or a false alarm.

In this work, we propose a novel approach for unit test generation based on the idea of carving unit tests out of system test executions. 
Elbaum et al~\cite{}\cite{}, which firstly present the idea of test carving, identify three types of approaches for carving:
state based carving, working on instances, aiming at recreating instances observed during system test execution based on heap state. 
Carving takes place on the object structure and identifies the relevant elements that define the state of an instance before and after a MUT to generate a test case. \xxx{check the paper again}.
Action based carving, which works by identifying and replaying only the method invocations that lead to a specific state of the CUT.
and Hybrid.
\xxx{Make sure it is clear they implemented and evaluated only 1}.
Elbaum and coahutors implemented and investigated only the first of these three approaches, and show \xxx{summarize the conclusion from the papers}.
In this work, instead we propose and to our knowledge implement, the first approach for test carving based on actions.
We show that, in line with the finding of Elbaum et al., the generated test cases implement the same behavior of system tests, are focused, \xxx{and faster} to execute than system tests (\xxx{add numbers here from the evaluation}).

\xxx{Summarize qualitatively the differences}
The two approaches, despite sharing the same underlying principle, requires different setup/infrastructures and produce different test cases.
\xxx{ provide an example code here of both}
%
The approach from Elabuam et all, requires an infrastucture for capturing and replaying the execution, which is based on serializing object instances.
\xxx{double check that the following is actually true !}
During the execution of system tests, instances are serialized after each method invocation and stored away. After carving, the differential unit tests
require the same infratructore for loading and restoring in the heap the object instances. This means that carved test cases cannot be execute outside the carving infrastructure. Additionally, carved test cases might be hard to read since the test cases hide the details of the objects being tests. 
As shown in Figure~\ref{fig:state-based-carving-example}, differential unit tests have a specific pattern: load a complex object from an XML file, 
invoke a method on this object, compare the resulting state to another XML file. Load and compare are opaque operations.

Notably, since this approach is based on object instance serialization and restore, it might (or not entirely) applicable for SUT which involve, non serializable objects (like file descriptors, network connections, connections to databased, etc), and SUT which have a distributed state. For example, it might be impossilble by means of pure statebased carving to recreate the state of an application which relies on a database (or even files) for persisting data.
Since de-serialization of the object instance, cannot recreate the inner state of the database (or file content).

At the contrary,  being based on action carving \abc does not suffer of those problems: once tests are carved, they do not require any specific infrastructure to be used. They in fact rely on the available testing infratructure (e.g. , JUnit and whatever test scaffolding the application used for system testing in the first place). This make carved tests ready to use. Second, carved tests are not opaque, but verbose (maybe they suffer of the opposite problem?).
That is, they explicitly replay the method calls to restore the original (or similar) application state (either this is distributed or local), and can be read.
%They follow the usual pattern, setup, execution \xxx{assertion}. 

In this paper, we do not focus on generating regression assertions, but during the carving we verify (using the same approach of Elbaum et al.) that the generate test cases lead to the same tests of the CUT before and after the test execution. Similarly, we obejseve that the return value of MUT produce the same results. Automatically generate regression assertions is a different problem than carving, hence we leave that for future work.

\paragraph{Contribution}
A novel approach to unit test case generation based via action-based test carving
An evalaution of this approach against test subjects which use Files and Databases to operate

\begin{figure*}[h!]
\centering
\includegraphics[width=\textwidth]{overview}
\caption{Overview of the \abc approach: \abc \emph{instruments} the application under test to inject tracing code; next, it \emph{executes the system tests} using the instrumented version of the application to generate a trace file and a copy of the instances processed during the execution; then, it \emph{carves} a set of unit tests that stress the application in the same way systems tests do out the trace execution, \emph{validate} that the unit tests to check they implement the same behavior of the system tests, and finally, it \emph{reduces} the size of the carved test suite using delta debugging.}\label{fig:overview}
\end{figure*}

\section{\abc Overview}
\xxx{A nicer intro better. Remark the meaning of action-based carving, which then clarify why we need data from dynamic executions of system tests to carve the tests.}

\abc implements action-based carving in four steps: instrumentation, tracing of system test execution, test carving, and test suite minimization. \xxx{what about delta debugging?} 

\paragraph{Instrumentation}
We instrument the application under test to inject code which traces the execution.
%
In particular, for each method invocation we trace the method being invoked, the owner of that method invocation unless the method is static, and the return value of the method invocation unless the method is void.
The method invocation owner is the object instance upon which the method is invoked.

Informations about method signature and method owner are taken \emph{before} the method is executed, while information about its return type are taken \emph{after} the execution.
%
We do so to avoid losing method ownership informations in case the control flow of the application is abruptly interrupted during the execution of a method, either because an exception is  thrown  or because the application terminates by invoking \texttt{System.exit}.
%, which prevents us to collect observereturn values and more importnatly ownership informations otherwise).

Instrumenting \emph{only} method calls is not enough to build a sufficiently complete trace of execution, that is, a trace which allows to rebuild the original execution of the application before carving int. 
Important informations, including assignments and access to fields, store and access to array elements, which are relevant to rebuild the data dependencies among method executions, would be missing from the trace of execution if we would track exclusively method calls.

To consider those information during the execution, we trace them as \emph{artificial} method invocations. 
For example, a statement like \texttt{A.f = v} is interpreted as an artificial method call to a static method 
\texttt{setField(String fieldName, Object fieldValue)} of an \xxx{ephemeral/fake} class \texttt{abc.Class}.
Similarly, a statement like \texttt{v = a[0]} is interpreted as an artificial method call \texttt{elementAt(int position)} of the array \texttt{a}.
%
Adopting this approach enables us to keep all the required information for test carving around and treat all \xxx{operations/statement} in the application in a standard and uniform way during carving.

Additionally, \emph{after} each method invocation we serialize the state of the return value and the state of the method invocation owner to files. Differently than the informations about method calls, which are mandatory for carving the test cases, such serialized state of the objects are optional and we use them to cross-check the correctness of our carved test cases.

In our prototypical implementation, we rely on Soot~\cite{} for instrumenting the application because the framework is rather intuitive, well supported, and we have previous experience with it; however, using Soot is by no means necessary to implement action-based test carving. As long as it can generate a complete trace of execution, any instrumentation technology can be used for this purpose. We rely on XStream~\cite{} to serialize object states into XML Files and persist the on disk because XStream is the de-facto library for object serialization in Java and the same library was used in the original work by Elbaum and co-authors to implement state-based carving.

\paragraph{System test execution and tracing}
We run the system tests using the instrumented code of the application under test to produce the execution trace and the dump of all the object instances processed during the execution. As common practice, before each system test we ensure the environment is not polluted and then we execute each system test in an isolate JVM, an execution mode supported by state of practice test execution frameworks~\cite{surefire-fork-isolate}. Additionally, we reduce this way the possibility of causing manifest test dependencies, flaky tests, and other problems related to polluted states which invalidated our analyses~\cite{Glicoric-or-Giory, Gambi,Bell,Ernt-DTD}.

\paragraph{Test Carving} 
Action-based test carving develops by identifying the method to carve and building a backward
slice which transitively includes all the method calls which potentially have an effect on the owner
of the method to carve and its parameters.
\abc does so by elaborating on three data structures that we derive from the trace of system test executions:
an execution flow graph, which captures the time-dependencies between method calls;
a data-dependency graph, which captures dependencies on data such as parameters, method owner, and return values;
and, a call-graph, which captures the structural dependencies of method calls and their nesting.

Action-based test carving not only carves the preconditions of the method under tests, but it also considers interactions with the external environment which might have an effect on the behavior of system under tests. Example of such interactions are accesses to the file system, databases, or remote services. Accessing the environment is typical done by means of specific interfaces, libraries and APIS. For instance, \texttt{java.util.Scanner} receives user inputs from the command line, classes inside the \texttt{java.io} and \texttt{java.nio} packages wrap calls to the underlying file system, and classes under the \texttt{java.sql} package enable database interactions. Since the list of the external interfaces is largely application specific, \abc enables developers to configure which external interface to consider and to specify additional external interfaces.
\abc includes interactions which potentially affect the behavior of the system under test by conservatively carving the calls to those  external interfaces which precede the call to the method to carve, thus ensuring that any precondition that tests have on the environment are met. Notably, this is impossible to achieve via standard state-based test carving, which controls only
the internal state of the application.

\paragraph{Test Validation}
\abc implements an additional validation step to check the validity of the carved tests. 
%
Some of the carved tests do not \emph{directly} invoke the method under carving; in this case,
despite the execution of those tests ``at some point'' produces the expected results, that is, the state of the class under test is the same observed during the execution of system tests, \abc report them as invalid and discard them.
%
The reason behind this choice is the observability of the results: Since the invocations 
 of the methods under test are not directly visible in the code of carve tests, there's no straightforward way
for developers to assert the behavior of the unit.

Other carved tests, because of improper definition of external interfaces or corner cases in the implementations of the application under test which \abc prototype does not handle, simply do not produce the expected result. 
We identify those tests by relying on the serialized object values that we collected during system test execution. From the XML dump of the serialized objects, \abc creates a set of test of basic assertions on the state of the unit under test after the methods to carve are invoked as well as on the return values they produce.
Those assertions, used in conjunction with typical implicit test assertions~\cite{randoop}, like uncaught exceptions, let us identify
 tests which produce an unexpected result and report them to the developers. We remove the failing tests from the carved test suite.
Since the assertions based on XML serialized object have the same drawback of the assertions generated by Elbaum and co-authors,
we remove the from the code of the carved tests before generating the final carved test suite. This way we ensure the creation of realistic executions, but give developers the freedom to create and use their own assertions. The generation of effective regression test assertions is an central topic in the domain of automatic test case generation, and we leave it for future work.

\paragraph{Test Minimization}
Since \abc adopts a conservative approach to include method calls which might affect the state and the behavior of the application under test, the resulting carved tests might be longer than necessary. 
For this reason, \abc performs a last step that aim to minimize the size of the test methods by identifying and removing method calls which do not change the state of the application or alter its behavior.
To do so, \abc implements a technique based on delta debugging~\cite{delta-debugging}: it starts from the method call right before the method under test, and moving backwards to the beginning of the test case, removes a statement from the test and executes it.
If the modified test does not fail despite one statement was removed, we conclude that that statement was not required in the scope of the carved test. To decide if the test passes or fails, we rely on the same assertions we used in the validation step.
By removing unnecessary methods calls from the carved tests we reduce their size to a minimum, which makes them possibly easier to read and faster to execute. 
%
Once again, we remove from the carved test suite those test  which after minimization implement exactly the same behavior.

\bigskip

At the end of this process, the generated test suite implements the same behavior of the system tests used to create the traces; however, instead of doing so by means of few long running system tests, it uses a multitude of shorter unit tests. This, in turns,
enables developers to implement traditional regression selection and test suite minimization activities, which are hardly possible for system tests, and let them focus to precise points in case of regression bugs.

\section{Action-based Test Carving with \abc}
\xxx{Focus here on the algorithms here}
Trace Parsing:

We parse the trace to create the following three main data structures for each system test (Figure + Example):
an execution flow graph which captures the temporal aspects, i.e., sequenciengn of the operations;
a call graph which captures the structural aspects, i.e., nesting /caller-calle dependenciens of the operations;
and, a data dependnecy graph which captures the data dependencies among the operations. Examples are ownership depednencies among an object instance and the methods called upon it, data dependencies on method parameters/arguments, and return values.

Carving is done in three logical steps: core carving, we carve the methods directly involved in the MUT; carving external interfaces, we carve methods that belongs to external interfaces, and finally,  including environment test setup. Carving steps are done conservatively by including all the calls that MIGHT influence the execution. This includes calls to CUT that precede MUT which might change its status, and calls to external interfaces that might change the status of the environemnt, for example, by creating or changing file or databased contents, or by login to external services, etc. 

Core carving: we start from the target MUT, and compute a backward slice by the computing the 
	intersection between the set of method invocations that happened BEFORE the target MUT, and the
	the method invocations which are reachable by means of data dependencies from the MUT. Those recursively include,
		all the calls which set up the CUT, and the calls which provide all the parameters of of the MUT.
	When it comes to providing parameters to functions, there might be two cases: the object is provided via its constructor and is modified by means of method calls, and the object is returned by another method calls. The latter for example is typical when application data are returned by Factory methods.
	To generate carve tests we consider all the possible combinations of these two cases for each parameter to the MUT, therefore we can create test cases like CODE1, where we completely disregards the application logic in favor of a direct constructions of dependencies, and like CODE2, where we rely on the actual code of the application under test to provide instances of required objects, as long as they correspond to behaviors observed during the execution of system tests.
	A final step, to avoid that by mix matching method invocations from different locations we end up calling the same methods multiple times, we use the call graph to compute subsumsion sets, that is, the set of method invoactions inside the carved tests, which are subsumed by method calls already included.
	This is illystrate in CODE 3. During this last step, it might happend that also the MUT is subseumed, thus removed, from the carved test.
	Since this violated the basic assumtpion of carved tests, that is, the MUT must be DIRECTLY called in the test, we reject those spurios tests.
	In theory, those tests are valid from the execution point of view, that is, the state of the application at the time the MUT is invoked is the correct one, 
	however, because of call nesting, there no easy (or any at all) way to extract the CUT and the return value of the MUT, making impossible to define correct assertions on them.
	\xxx{How do we handle the cases in which the same object, i.e., String buffer, is called multiple times? Do we take the last invocation or the one which is closer to MUT? If this becomes too complex. we might omit this details from the explanation.}

Algorithm ALGO summarize the action based test carving algorithm.

Carving of External interfaces (conservative): Users list the external interfaces upon which the SUT depends. Common interfaces include java.io.Files, java.nio.Paths which act as interface/api to the underlying File System, classed under java.sql which acts as interface to Sql databases, and java.util.Scanner, which act as interface to the user (via the Console/command line). We carve invocations on those external interfaces by recursively carving all the invocations to them which happens before MUT and which are NOT yet included in the CORE carving, to avoid calling external interfaces multiple times. For carving, we proceed similarly to Core carving \xxx{Double check the algorithm if we still consider combination of inputs}.

Including test setup methods (conservative): We do not instrument nor carve test setup methods, like the one commonly executed inside @Before.
The motivation here is that those methods configure the test environment for the system tests, i.e., they do not belong to the application logic,
but rather to system tests logic. Nevertheless, carving tests (might) require those methods to be in place. For example, a typical setup method drop and recreate the mysql database. This ensures that system tests start from a known state of the database. Since tests carved from the system tests rely on the same assumptions, we need to include and replicate the same setup before executing them. Again we adopt the conservative approach of including all the required setup calls to generate carving tests. This has the benefit to ensure that the right preconditions are in place, but has the drawback to increase the execution overhead of the carved test. Imagine the case a carved test performs a basic check on an input, i.e., the input shall be greater than 0. If this test was carve out of a system test which requires a databased filled with test data, our conservative approach will recreate the databased also for the carved test, unnecessarily. Because of this, we will proceed by minimizing the test cases and removing all the unnecessary method invocations from the carved test.

The last step is the actual test generation where we take carved and minimized tests are produce executable java (JUnit) tests. We use open source library to generate the java code from our internal representation based on jimple/soot.


Non carvable methods

\xxx{If it turns out that the coverage is the same, we describe this into the limitations: non=carvable methods}
Corner cases ?  Multiple return methods, access via fields/getters, etc.
The basic approach to carve by following data deps and including all the calls, for then subsming them, has the problem
that for some objects we partially subsume their calls, this create unsafe operations such that some calls on objects are made
directly from the test code, while the object initialization is done elsewhere (inside the subsumer call). Leaving the non-subsumed calls in the test code dangling (this raise NPE during test execution).
There are different strategies to deal with this: one can recreate the object initialization and break the dependecies on the subsumer object. That is, we call once again the constructor of the "dangling" instace and repeat its initialization. 
This strategy is simple to implement because we have already all the information to perform the carving, however, 
it has the downside of repeating the execution of some code, which might or might not problematic dependending on the application, but definitively consists in executing more code than necessarily. Another alternative, that we implement in \abc, consists in recovering this situation by substituting the "dangling" calls with calls of the subsuming owners. This for example, is the case of an object which has a field that cause the unsafe access. For those, we look in the method invocations performe don the owner, if we can find replacement calls for the unsafe ones. This way by invoking the replacement calls we set the state of the object in a safe way.
In fact the object, will be accessed later by means of the getter.



Test methods minimization via Delta debugging, i.e.,  data-aware delta debuggin ? optimized

\section{Limitations and possible improvements}

\subsection{Non-carvable unit tests}
There are situations in which \abc cannot carve unit tests "at level-0/object interface". For example, \abc cannot carve methods which have "inner/outer" dependencies. That is, methods mX, which are called by outer methods mY, but at the same time requires inputs from the owner of mY. In this case, carving mX requires to include mY and all the methods called on mY owner, which eventually subsumes mX and therefor invalidate the basic property of (our) unit tests, which requires the CUT and the MUT to be directly visible/called in the test code. 

There might be different strategies to deal with this, but none necessarily general and applicable to all cases.
For example, we can break method dependencies: \abc might rely on developer provided information to break the dependency between mX and the owner of mY, such that it is not included in the carved test. This is for example the case of having a constant to be provided via a method by mY owner. In this case, "any" instance of that object would work the same. We might break the dependency by using an hybrid approach to provide a copy of the required value, under the assumption of not having side effects on the owner of mX. This for example is the case where the value is a primitive value or a string. 

A completely different strategy to test that specific method invocation consists in violating the basic property of level-0 test cases, and generate test cases which invoke the method subsuming mX but capturing and exposing the CUT and the result of the MUT, for example by smart checkpointing or mocking, or by directly injecting into the application code assertions to be invoked right after the target MUT. This, however, requires a smart infrastructure, similar to a debugger, to be in place during test execution to capture the precise moment the target MUT is invoked (simply imagine that MUT is invoked inside a for loop, and we are interested in generating a test case which check the 2 invocation of that method !). The development of such a smart testing infrastructure is part of our ongoing effort for carving test cases out system tests executions. 

Remarkably, the fact \abc cannot carve unit tests, might be also used to investigate the testability of an application. Infact, in many cases, failing to carve "inner/outer" test cases, is not always a problem with \abc but with the application itself, which simply has an hard-to test/bad design. Studying how \abc can identify \xxx{code smells} to assess the testability of an application is part of our on-going research effort.

\subsection{Missing observations}

\paragraph{Missing observations due to Exceptional behaviors}

When an exception is thrown and not caught methods do not complete regularly, as a consequence, we do not trace their returns and the trace might be incomplete. The trace cannot be correctly parsed since we expect to observe a return from each method invocation.  Therefore, we might not correctly carve some tests.

\paragraph{Missing observations due to partial instrumentation}
Difficulty to reconstruct objects which cannot be observed. Trace might not include the calls which generate an object instance, therefore a test which uses that cannot be carve. For example, tomcat provides httprequest object, we are interesting to carve the calls to our JSP/Servlets, plus we do not instrument (atm) Tomcat itself. So there's the need to get to the httpRequest object in the first place. This can be addressed by instrumenting tomcat (huge traces, huge overhead) or by extending \abc to implement an hybrid approach to test carving: the idea would be to use serialized versions of the objects for which we cannot observe setup and configuration (state-base carving), and then rely on action based carving to carve out the other tests. Carved objects might be then reconstructed using a search approach similar to the one proposed by Denaro et al~\cite{} or by Gligoric et al.~\cite{}. Integration with those approaches and extension of \abc are left for future work.

Implementation does not cover all the cases? Can be addressed by extending the instrumentation to cover them.
For example, we can trap exceptions and invoke artificial method calls. We might extend the scope of the instrumentation to capture object initialization. Those can be done without extending the approach but just the implementation. Alternatively, we can extend the approach to become hybrid.

\paragraph{Scalability}
If we face long execution of system tests which generate huge traces traces of sheer size, \abc requires to consider many invocations inside its carved  tests. This is a consequence of conservatively include ALL the method calls to interested objects observed BEFORE the invocation of MUT.
This might raise scalability concerns, because the carving time becomes larger as the trace becomes larger.

To address scalability we employ several mechanisms:
we separately carve test cases for each system test, and we heavily rely on caching.
For example, in the context of the same system test, many objects share the same preconditions either because they depend on
the same input objects or because they require the same invocations to external interfaces. In those cases, instead of 
re-carving the same over and over, we provide the preconditions using the cache.

This however, has little effect for situations in which the preconditions do not exactly match. For example, if the same method is invoked in different
times, the later invocation of the same method include the same preconditions of the previous method calls, but also inculde all the method calls on the objects and external interfaces that happened between them. One strategy that can be employed in this case, which is similar to what Randoop does to build randomized sequences, is to carve preconditions incrementally. So instead of rebuilding the preconditions from scratch, we can rely on 
%
That is, by forcing the carving of methods according to the order of appearance of MUT, we can rely on caching partial setups.
\xxx{Maybe we can try this out if we have time before the submission?}

\paragraph{Scalability - Too many test cases}
Another aspect related to scalability is the generation of many (too many) carve tests. On one side, this give developers the freedom to chose which carved test to select and expose the diversity of the behaviors stressed by the application, on the other side, many of the carved test cases are similar to one another.

We partially address this by filtering out test cases that are similar (at JIMPLE level) at the end of test carving, and to some extend during test case generation. However, we perform only basic checks, mostly syntactical, and those might not be as effective as smarter similarity metrics.
We also perform a test suite minimization based on coverage (and delta debugging?) This is effective to remove tests which do not contribute to coverage, but completely disergards test case diversity/richness

Minimization of the test suite, but also identification of similar test cases, that is test cases which stress the CUT in the same way, despite they setup/configure the environment and the preconditions in different ways. 
\xxx{Maybe some static analysis can help here, maybe we can rely on the heuristic that if the input values to MUT, its state of the CUT before and after invoking MUT are the same, we might assume the tests are similar and pick only one?}


\subsection{Instrumentation overhead}
Instrumentation dumps everything, huge overhead. Filter the instrumentation to dump only application objects or objects returned from invocations from MUTs.

\section{Evaluation}

\textbf{Goal of the evaluation} 

Show the usefulness of test carving by means of showing that 
carved tests stress the SUT in the same way systems test do, but at the unit level. 
That is we do not lose testing power/miss behaviors by using carved tests.
%
We illustrate this by showing how code coverage achieved by system and carved tests is the same.

\paragraph{Test case selection reduction}
Carve tests are more focused than system tests, hence, in a regression setting we do not necessarily need 
to execute all of them. This is not true in general for system tests which, by involving the entire system, end up in using (hence depending on)
all the system classes (or at least a great deal of it).
%
We illustrate this by modifying the system under test (using bogus, non-logic-modifing modificaitons) and counting how many system and carved
tests are executed after each modification. In particular, we show that modification of few classes already caused all system tests to be triggered,
while only a relevant subset of carved tests are re-executed.

\paragraph{Test suite reduction}
Carved tests are much more than system test, many of the carved test stress the same code of the application, although under different execution conditions. Therefore, they achieve the same coverage, and can be easily eliminated if one is interested in achiving a regression test suite optimized towards coverage.
%
We illustrate this by performing a test suite reductions on carved tests and showing that we can remove many (up to 80\%) of the carved tests, compared to removing barely 10\% of system tests.

Additionally, even reduced test suite, show benefits for RTS.

\paragraph{Overhead of Delta Debugging}.
Delta debugging -> this one becomes part of the evaluation. Since we adopt a conservative approach to include all the calls which might be state chaging,
we are interesting to investigate if this is "a bad thing". How many of those actions are superfluous and can be safely removed?
This also gives us a hint on if/how it would be beneficial to use delta debugging to minimize the carved tests cases.

For doing so, we implemente a delta debugging appraoch to remove and test each action, and compare the results on execution time, coverage and regression test selection of original and minimized test to see.
	generate validation instruction based on XML serialized state. We do not literally compare the serialized state, because some objects cannot be serialized in a meaningful way (i.e., open data base connections, file descriptors, streams) and some objects are parameters of the tests and are injected as dependencies during testing. For example, temporary working dir, date objects relative to current date, and other data that are generated randomly at the beginning of each test (and never changed during the execution?). 
	
Note: our apporach to delta debugging is naive, we simply try out all the instructions, however, we can also optimize the process by considering which method calls we included inside the tests because their were deemed necessary versus calls that we included because we'have been conservative. Additionally, we do not have to necessarily try out all the statement that we included for being conservative. Imagine we include method X, to call it, we needed to include all its preconditions. At this point, the moment we remove X, we can automatically remove all its preconditions - unless those are shared among other methods. This saves time, since we do not need to delta debug also the instructions needed to setup a given (and already proven useless) method X.
	
	\xxx{Give examples here on Employee and HotelReservation ? this means we need to introduce them as motivating scenarios?}
	
	Otherwise, we cannot remove dependencies correctly. \xxx{This requires an example}.
	We compare both the return value of the MUT if any, and the state of CUT.
	
	Starting from the instruction before MUT and going backwards, we remove one instruction, repeat the test execution, and check that the test passes. That is, no unhandled exceptions and no failed assertions.

	We assume a reset environment function, i.e., wipe out the database, is provided by the user to avoid polluting, and invalidating the result of delta debugging. Note that the carved tests instead do not require this, and are correct by design.
	
	\xxx{How much do we save in terms of instructions? and execution time?}

This is simple but effective solutions, however, it incurrs in a huge performance overhead because each carved test must be re-executed a number of times equals to the amount of instructions it contains. Optimization based on static analysis and data-flow analysis might be used here to identify cluster of invocations that belong together and possibly can be removed or left in the carved test all together. However, this is out of the scope of this paper and will be addressed in future work.
	\xxx{How much does it cost to run delta debugging?}

Benefits of Delta Debugging. Since delta debugging is expensive, we want to investigate/assess its cost/benefits. We do so by comparing execution time and number of instructions before and after minimization, as well as regression test selection.
%
Note that in this cases the evaluation is biased by the choice of the test subject, which do not involve external services (which might be slow and expensive to use), but only locally available resources.

\xxx{Maybe seel this thing of delta debugging as part of the evaluation to show that, because delta debugging does not remove many instructions, 
then the carved tests are already good, cannot be improved in that sense. Note that this accounts for the instructions not the method invocations executed !}

Overhead of carving, scalability of the approach. Longer execution of system tests generate larger traces, which requires to consider more and more invocations (since we conservatively include ALL the method calls to interested objects)

We are also interested in assessing the overhead of the overall approach to derive conclusions on its practical applicability.
For this, we measure and report execution and slow down factors of instrumentation, tracing, carving, test generation, and delta debugging.

\xxx{Carving time per method? per class?}
\xxx{Carving time vs Place in the trace/size of the trace/amount of system tests}
\xxx{Carving time vs type}
\xxx{Carving time vs number of arguments}
\xxx{Carving time vs Object to be carved}
\xxx{Carving time vs Type/System tests}

Overhead of Minimization ?
\xxx{size of test suites}
\xxx{size of test suites}


I suspect that delta debugging might shows benefits for those classes which do not involve or depend on external interfaces,
and are used frequently.
For example HotelView has barely no logic, but to test it there's a lot of setup that can be removed.
Really relevant?


\xxx{Carved tests are faster to execute than system tests.}
\xxx{Maybe we can show this per class under test: we need to execute only a subset of tests}
\xxx{With our test subjects, we cannot really shows this, since the time}

\subsection{Test Subjects}

Employee project.
Interactive command line utility to manage employee and salaries. It works by creating, reading, and deleting files.
\xxx{check if the same (or similar) to original carving work}
\xxx{Add stats?}

HotelReservation System project.
Interactive command line utility to handle an hotel reservation system. It uses mysql backend to manage the state of the applications.
\xxx{Add stats?}

\subsection{Experimental Settings}
System tests. The test subjects do not come with system tests, so I created them manually.
System tests work by resetting the state of the system before each test, mocking input commands, and cleaning up the state of the system
to avoid pollution~\cite{pollution}.
Additionally, system tests set up expectations on System.exit to avoid system and carved tests to abruptly halting the execution.

For the  Employee projects, I rely on temporary folders and predefined file contents.
For the HotelReservation, I rely on truncating the database and recreate it as expected (either empty, or with predefined content).

\xxx{How system tests are defined? Ideally I would like to create them by following a standard technique, in practice, I will motivate my choice
by saying I creating them to cover as much application state as possible, in a black box style.
That is, I consider "each" page of the application as state and navigate the application states by means of different input sequences.
The name should be coverage-transition? Check on the software testing class}

% Maybe a bit to verbose
To answer RQ1 and RQ2, I execute the system tests and measure the achieved coverage;
Next, I instrument the application,  repeat the execution of the same system tests to collect the trace, 
and from the trace carve the tests out. I generate carved tests and minimized carved tests using delta debugging.
Finally, I execute the carved and minimized carved tests to measure coverage, time to carve tests, 
time to generate tests, and time to minimize test via delta debugging.

To answer RQ3 I proceed as follow. 
Usefulness of Carve Tests:
I execute the Ekstazi tool~\cite{} on the original version of the code separately for system and carved tests.
Then, I randomly change a number of source files n (n={1, \ldots, N}, N is the total amount of application files (no tests, no Main)) in the project.
Next, I re-execute Ekstazi separately for ssytem and carved tests, but keeping the same modification , and collect information on how many system and carved tests where executed upon each modification.
Ekstazi will select only the tests which are needed to be re-execute to test the modification.
The modification is bogus, we do not modify for simplicity interfaces, but only implementation classes to avoid compilation problems.
We repeat the process for 20 times and for n from 1 to N (N is application classes)

\subsection{Results}

%\begin{table*}
%\centering
%\caption{Summary of Test Carving}
%\begin{tabular}{l l l l l l l l l}
%Project & System Tests &  Carved Tests & Class Coverage & Statement Coverage & Branch Coverage & time SYS &  time Carv &  time Carv MIN\\
%Employee & 7  & 87 &  100\% & 35\% & 14\%  & 0.045 & 0.081 & --\\
%Hotel Reservation & 5 & 145 & 100\% & 41 \% & 29\% & 0.615 & 7.78 & --\\
%\end{tabular}
%\end{table*}

\begin{table*}
\centering
\caption{Summary of Test Carving.}
\begin{tabular}{l l
			r r r
			r r r
			r r r
			r r r 
			r}
\toprule
\multicolumn{2}{c}{Project} & 
	\multicolumn{3}{c}{Tests (\#)} &
	\multicolumn{3}{c}{Execution Time (sec)} &
	\multicolumn{3}{c}{Stmt. Coverage (\%)} &
	\multicolumn{3}{c}{Branch Coverage (\%)} &
	\multicolumn{1}{c}{Trace Size (\#)} \\
	\cmidrule(rl){1-2}
	\cmidrule(rl){3-5}
	\cmidrule(rl){6-8}
	\cmidrule(rl){9-11}
	\cmidrule(rl){12-14}
	
	
	\multicolumn{1}{c}{name} & \multicolumn{1}{c}{feature} &
	\multicolumn{1}{c}{\textbf{S}} & \multicolumn{1}{c}{\textbf{C}} & \multicolumn{1}{c}{\textbf{M}} & 
	\multicolumn{1}{c}{\textbf{S}} & \multicolumn{1}{c}{\textbf{C}} & \multicolumn{1}{c}{\textbf{M}} & 
	\multicolumn{1}{c}{\textbf{S}} & \multicolumn{1}{c}{\textbf{C}} & \multicolumn{1}{c}{\textbf{M}} &
	\multicolumn{1}{c}{\textbf{S}} & \multicolumn{1}{c}{\textbf{C}} & \multicolumn{1}{c}{\textbf{M}} &
	 \\
\midrule
Employee & File system &
	11  & 464 & 15 & 
	5.1 & 2.1 & 0.17 & 
	73 & 73 & 73 &
	44 & 44 & 44 &
	5,470 \\
Hotel Reservation & SQL database & 
	17  & 503 & -- & 
	12.5 & -- & -- & 
	82 & -- & -- &
	64 & -- & -- &
	35,820 \\

\bottomrule
\end{tabular} \\
\flushleft{The table reports an overview of the results achieved by \abc.
The \textbf{S} label identifies results for system tests, while \textbf{C} and \textbf{M} identify respectively the results for the carved and minimized test suites.
\xxx{Check if the achieved coverage is close to the maximum coverage that can be using only system tests!}
}
%\xxx{20 repetitions for the execution time}s
%	\multicolumn{1}{c}{Carving Overhead} & \multicolumn{1}{c}{Reduction} & \multicolumn{1}{c}{Delta Debugging}
%& 4,106 
%\xxx{Repeat the analysis and that the average execution time}
\end{table*}

\begin{figure*}[t]
\centering
\begin{minipage}[b]{.45\textwidth}
\includegraphics[width=\columnwidth]{figures/employee-rts-size}
\caption{Test Regression Selection - Employee}
\xxx{Make a Box Plot here instead. Lines connecting the average values, but boxes to show min/max ? Maybe consider splitting the plot in differen plots one for each approach, so it might be easy to compare?}
\end{minipage}\hfill
\begin{minipage}[b]{.45\textwidth}
\includegraphics[width=\columnwidth]{figures/hotelme-rts-size}
\caption{Test Regression Selection - Hotel Reservation}
\end{minipage}
\end{figure*}


\begin{figure*}[t]
\centering
\begin{minipage}[b]{.45\textwidth}
\includegraphics[width=\columnwidth]{figures/employee-rts-time}
\caption{Test Regression Selection - Execution time - Employee}
\xxx{Not sure this is relevant - update the system tests to last longer and explore more app behaviors}
\end{minipage}\hfill
\begin{minipage}[b]{.45\textwidth}
\includegraphics[width=\columnwidth]{figures/hotelme-rts-time}
\caption{Test Regression Selection  - Execution time -  Hotel Reservation}
\xxx{Not sure this is relevant - update the system tests to last longer and explore more app behaviors}
\end{minipage}
\end{figure*}

\subsection{Observation}
\xxx{For employee, Why the number of carved tests keep changing for hotel me?}
\xxx{For some }


\subsection{Discussion}
Coverage is the same

Execution time is larger for carved unit tests fro projects with simple setup.

We can generate tests for all the classes

Does minimization make sense?


Should I keep or not the test setup calls intact? Should I instead disassemble those as well ?!

Delta Debugging dominates the test generation time.
There's no difference in RST with and without minimized test cases.
For SOME REASON the minimized tests it takes more time to execute than the normal ones..
%
How many tests minimized? and how much?
%Employee: 87, 13 minimized (~14\%), those have an avg minimization of 95.55\%

% Is there a differnce in RTS.
Employee: there no difference in RTS, exactly the same tests are selected upon the same modifications.

\subsection{Limitations}
Action based carving need to "see" how object instances are created, otherwise might not be able to perform the carving because there's no way for the carver to reconstruct the object instance.

We observed this problem for example with test setup calls?

\subsection{Discussion}

Complex programs with GUI and WebApplications call for a hybrid approach: the hybrid approach provides (if possible) the parameters to the entry points of the application, for example, the servlet or the logic behind the GUI, which otherwise cannot observe the object creation thus replicate it.
With an hybrid approach, one might serialize the parameters of the calls and reload them to spin off the carving. E.g., one can define mocks that provide http requests, fill in with values objserved during the execution.

\end{document}