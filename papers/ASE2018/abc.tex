\documentclass[10pt,conference]{IEEEtran}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}

\usepackage{graphicx}

\usepackage{textcomp}
\usepackage{xspace}
\usepackage{booktabs}



\usepackage[table]{xcolor}
\usepackage{paralist}
\usepackage{url}

\usepackage{boxedminipage}
\usepackage{enumitem}
\usepackage{graphicx}  
\graphicspath{
	{figures/}
}
\DeclareGraphicsExtensions{.pdf,.jpeg,.png}


% Complex \xxx for making notes of things to do.  Use \xxx{...} for general
% notes, and \xxx[who]{...} if you want to blame someone in particular.
% Puts text in brackets and in bold font, and normally adds a marginpar
% with the text ``xxx'' so that it is easy to find.  On the other hand, if
% the comment is in a minipage, figure, or caption, the xxx goes in the text,
% because marginpars are not possible in these situations.
{\makeatletter
 \gdef\xxxmark{%
   \expandafter\ifx\csname @mpargs\endcsname\relax % in minipage?
     \expandafter\ifx\csname @captype\endcsname\relax % in figure/caption?
       \marginpar{\textcolor{red}{xxx~}}% not in a caption or minipage, can use marginpar
     \else
       \textcolor{red}{xxx~}% notice trailing space
     \fi
   \else
     \textcolor{red}{xxx~}% notice trailing space
   \fi}
 \gdef\xxx{\@ifnextchar[\xxx@lab\xxx@nolab}
 \long\gdef\xxx@lab[#1]#2{{\bf [\xxxmark \textcolor{red}{#2} ---{\sc #1}]}}
 \long\gdef\xxx@nolab#1{{\bf [\xxxmark \textcolor{red}{#1}]}}
 % This turns them off:
% \long\gdef\xxx@lab[#1]#2{}\long\gdef\xxx@nolab#1{}%
}

\newcommand{\abc}{\textsf{ABC}\xspace}
\title{ABC: Action-Based Test Carving}

\author{\IEEEauthorblockN{Alessio Gambi}
\IEEEauthorblockA{\textit{University of Passau} \\
%\textit{name of organization (of Aff.)}\\
Passau, Germany \\
alessio.gambi@uni-passau.de}%
}


\begin{document}

\maketitle

\begin{abstract}
\end{abstract}

\section{Intro and motivation}
Importance of tests, classes of tests and their pro/cons.
In this paper we focus on system and unit tests, specifically in their use for regression.

System tests are wide scoped, but "real" (~\cite{exysts}).
However, they take long time to complete because they need to setup/deploy the real system, and interact with it.
A common setup is the one to start the SUT and one or more "clients" which interacts with it.
System tests can be automatically generated, for example, via fuzzying and random exploration, assuming a proper driver/infrastructure is available.
For example, this is commonly the case for GUI (Exysts),  Web Application (with crawlers and Selenium) and Apps (Monkey).
%
In general, however, system tests for regression are setup manually: a designer/testers creates them to implement 
specific scenarios and test specific features (~\cite{TDD}-controlla nel libro).
%
For example, login into a system and buy a product, or rent a room. (Maybe better the latter so we keep around this for the test subject). 
%
This "ensures" to capture, during development, refactoring, etc. changes to the code which breaks funcitonalities. 

At the opposite spectrum, there's unit tests. Unit tests are focused, can be easily generated automatically (Evosutie and Randoop), and can achieve high coverage. However, (automatically) generated unit tests they might be "detached" from the reality, that is, they might implement executions which cannot be observed in real life (cite out paper at AST). Units test might turn out to  "waste" precious testing time.
Despite they might capture dangerous or problematic behavior of the SUT (e.g., an uncaught NPE), those problems might be only potential, and it might hard for developers to understand if the failing of a unit test is real or a false alarm.

In this work, we propose a novel approach for unit test generation based on the idea of carving unit tests out of system test executions. 
Elbaum et al~\cite{}\cite{}, which firstly present the idea of test carving, identify three types of approaches for carving:
state based carving, working on instances, aiming at recreating instances observed during system test execution based on heap state. 
Carving takes place on the object structure and identifies the relevant elements that define the state of an instance before and after a MUT to generate a test case. \xxx{check the paper again}.
Action based carving, which works by identifying and replaying only the method invocations that lead to a specific state of the CUT.
and Hybrid.
\xxx{Make sure it is clear they implemented and evaluated only 1}.
Elbaum and coahutors implemented and investigated only the first of these three approaches, and show \xxx{summarize the conclusion from the papers}.
In this work, instead we propose and to our knowledge implement, the first approach for test carving based on actions.
We show that, in line with the finding of Elbaum et al., the generated test cases implement the same behavior of system tests, are focused, \xxx{and faster} to execute than system tests (\xxx{add numbers here from the evaluation}).

\xxx{Summarize qualitatively the differences}
The two approaches, despite sharing the same underlying principle, requires different setup/infrastructures and produce different test cases.
\xxx{ provide an example code here of both}
%
The approach from Elabuam et all, requires an infrastucture for capturing and replaying the execution, which is based on serializing object instances.
\xxx{double check that the following is actually true !}
During the execution of system tests, instances are serialized after each method invocation and stored away. After carving, the differential unit tests
require the same infratructore for loading and restoring in the heap the object instances. This means that carved test cases cannot be execute outside the carving infrastructure. Additionally, carved test cases might be hard to read since the test cases hide the details of the objects being tests. 
As shown in Figure~\ref{fig:state-based-carving-example}, differential unit tests have a specific pattern: load a complex object from an XML file, 
invoke a method on this object, compare the resulting state to another XML file. Load and compare are opaque operations.

Notably, since this approach is based on object instance serialization and restore, it might (or not entirely) applicable for SUT which involve, non serializable objects (like file descriptors, network connections, connections to databased, etc), and SUT which have a distributed state. For example, it might be impossilble by means of pure statebased carving to recreate the state of an application which relies on a database (or even files) for persisting data.
Since de-serialization of the object instance, cannot recreate the inner state of the database (or file content).

At the contrary,  being based on action carving \abc does not suffer of those problems: once tests are carved, they do not require any specific infrastructure to be used. They in fact rely on the available testing infratructure (e.g. , JUnit and whatever test scaffolding the application used for system testing in the first place). This make carved tests ready to use. Second, carved tests are not opaque, but verbose (maybe they suffer of the opposite problem?).
That is, they explicitly replay the method calls to restore the original (or similar) application state (either this is distributed or local), and can be read.
%They follow the usual pattern, setup, execution \xxx{assertion}. 

In this paper, we do not focus on generating regression assertions, but during the carving we verify (using the same approach of Elbaum et al.) that the generate test cases lead to the same tests of the CUT before and after the test execution. Similarly, we obejseve that the return value of MUT produce the same results. Automatically generate regression assertions is a different problem than carving, hence we leave that for future work.

\paragraph{Contribution}
A novel approach to unit test case generation based via action-based test carving
An evalaution of this approach against test subjects which use Files and Databases to operate

\begin{figure*}[h!]
\centering
\includegraphics[width=\textwidth]{overview}
\caption{Overview of the \abc approach. \abc develops in four main stages: it \emph{instruments} the application under test to inject tracing code; next, it \emph{executes the system tests} using the instrumented version of the application to generate a trace file and to dump a copy of the objects processed during the execution; then, it \emph{carves} a set of unit tests that stress the application in the same way systems tests do out the trace execution; and finally, it \emph{reduces} the size of the carved test suite while preserving code coverage.}\label{fig:overview}
\xxx{Move this to page 2}
\end{figure*}

\section{Test Carving with \abc}
\xxx{A nicer intro better. Remark the meaning of action-based carving, which then clarify why we need data from dynamic executions of system tests to carve the tests.}

\abc implements action-based carving in four steps: instrumentation, tracing of system test execution, test carving, and test suite minimization. \xxx{what about delta debugging?} 

\paragraph{Instrumentation}
We instrument the application under test to inject code which traces the execution.
%
In particular, for each method invocation we trace the method being invoked, the owner of that method invocation unless the method is static, and the return value of the method invocation unless the method is void.
The method invocation owner is the object instance upon which the method is invoked.

Informations about method signature and method owner are taken \emph{before} the method is executed, while information about its return type are taken \emph{after} the execution.
%
We do so to avoid losing method ownership informations in case the control flow of the application is abruptly interrupted during the execution of a method, either because an exception is  thrown  or because the application terminates by invoking \texttt{System.exit}.
%, which prevents us to collect observereturn values and more importnatly ownership informations otherwise).

Instrumenting \emph{only} method calls is not enough to build a sufficiently complete trace of execution, that is, a trace which allows to rebuild the original execution of the application before carving int. 
Important informations, including assignments and access to fields, store and access to array elements, which are relevant to rebuild the data dependencies among method executions, would be missing from the trace of execution if we would track exclusively method calls.

To consider those information during the execution, we trace them as \emph{artificial} method invocations. The benefits of adopting this approach are, one the one hand, keeping critical information for carving around, and on the other hand, treating  standard and artificial method invocations in a uniform way during carving.

Additionally, \emph{after} each method invocation we serialize the state of the return value and the state of the method invocation owner to files.
%
We elaborate the information about method executions and objects involved in them for carving the test cases, while we use  the serialized state of the objects to cross-check the correctness of the carved test cases.

In our prototypical implementation, we rely on Soot~\cite{} for instrumenting the application because the framework is rather intuitive, well supported, and we have previous experience with it; however, using Soot is by no means necessarily to implement action-based test carving. As long as it can generate a complete trace of execution, any instrumentation technology can be used for this purpose.

\textbf{Alessio here}

\paragraph{System test execution and tracing}
We run the available system tests using the instrumented code of the application to produce the execution trace.

for each of them, as well as a set of xlm files which contain the serialized state of the objects used during the execution.

Carving:

Trace Parsing:

We parse the trace to create the following three main data structures for each system test (Figure + Example):
an execution flow graph which captures the temporal aspects, i.e., sequenciengn of the operations;
a call graph which captures the structural aspects, i.e., nesting /caller-calle dependenciens of the operations;
and, a data dependnecy graph which captures the data dependencies among the operations. Examples are ownership depednencies among an object instance and the methods called upon it, data dependencies on method parameters/arguments, and return values.

Carving is done in three logical steps: core carving, we carve the methods directly involved in the MUT; carving external interfaces, we carve methods that belongs to external interfaces, and finally,  including environment test setup. Carving steps are done conservatively by including all the calls that MIGHT influence the execution. This includes calls to CUT that precede MUT which might change its status, and calls to external interfaces that might change the status of the environemnt, for example, by creating or changing file or databased contents, or by login to external services, etc. 

Core carving: we start from the target MUT, and compute a backward slice by the computing the 
	intersection between the set of method invocations that happened BEFORE the target MUT, and the
	the method invocations which are reachable by means of data dependencies from the MUT. Those recursively include,
		all the calls which set up the CUT, and the calls which provide all the parameters of of the MUT.
	When it comes to providing parameters to functions, there might be two cases: the object is provided via its constructor and is modified by means of method calls, and the object is returned by another method calls. The latter for example is typical when application data are returned by Factory methods.
	To generate carve tests we consider all the possible combinations of these two cases for each parameter to the MUT, therefore we can create test cases like CODE1, where we completely disregards the application logic in favor of a direct constructions of dependencies, and like CODE2, where we rely on the actual code of the application under test to provide instances of required objects, as long as they correspond to behaviors observed during the execution of system tests.
	A final step, to avoid that by mix matching method invocations from different locations we end up calling the same methods multiple times, we use the call graph to compute subsumsion sets, that is, the set of method invoactions inside the carved tests, which are subsumed by method calls already included.
	This is illystrate in CODE 3. During this last step, it might happend that also the MUT is subseumed, thus removed, from the carved test.
	Since this violated the basic assumtpion of carved tests, that is, the MUT must be DIRECTLY called in the test, we reject those spurios tests.
	In theory, those tests are valid from the execution point of view, that is, the state of the application at the time the MUT is invoked is the correct one, 
	however, because of call nesting, there no easy (or any at all) way to extract the CUT and the return value of the MUT, making impossible to define correct assertions on them.
	\xxx{How do we handle the cases in which the same object, i.e., String buffer, is called multiple times? Do we take the last invocation or the one which is closer to MUT? If this becomes too complex. we might omit this details from the explanation.}

Algorithm ALGO summarize the action based test carving algorithm.

Carving of External interfaces (conservative): Users list the external interfaces upon which the SUT depends. Common interfaces include java.io.Files, java.nio.Paths which act as interface/api to the underlying File System, classed under java.sql which acts as interface to Sql databases, and java.util.Scanner, which act as interface to the user (via the Console/command line). We carve invocations on those external interfaces by recursively carving all the invocations to them which happens before MUT and which are NOT yet included in the CORE carving, to avoid calling external interfaces multiple times. For carving, we proceed similarly to Core carving \xxx{Double check the algorithm if we still consider combination of inputs}.

Including test setup methods (conservative): We do not instrument nor carve test setup methods, like the one commonly executed inside @Before.
The motivation here is that those methods configure the test environment for the system tests, i.e., they do not belong to the application logic,
but rather to system tests logic. Nevertheless, carving tests (might) require those methods to be in place. For example, a typical setup method drop and recreate the mysql database. This ensures that system tests start from a known state of the database. Since tests carved from the system tests rely on the same assumptions, we need to include and replicate the same setup before executing them. Again we adopt the conservative approach of including all the required setup calls to generate carving tests. This has the benefit to ensure that the right preconditions are in place, but has the drawback to increase the execution overhead of the carved test. Imagine the case a carved test performs a basic check on an input, i.e., the input shall be greater than 0. If this test was carve out of a system test which requires a databased filled with test data, our conservative approach will recreate the databased also for the carved test, unnecessarily. Because of this, we will proceed by minimizing the test cases and removing all the unnecessary method invocations from the carved test.


The last step is the actual test generation where we take carved and minimized tests are produce executable java (JUnit) tests. We use open source library to generate the java code from our internal representation based on jimple/soot.

\section{Evaluation}

\textbf{Goal of the evaluation} 

Show the usefulness of test carving by means of showing that 
carved tests stress the SUT in the same way systems test do, but at the unit level. 
That is we do not lose testing power/miss behaviors by using carved tests.
%
We illustrate this by showing how code coverage achieved by system and carved tests is the same.

Carve tests are more focused than system tests, hence, in a regression setting we do not necessarily need 
to execute all of them. This is not true in general for system tests which, by involving the entire system, end up in using (hence depending on)
all the system classes (or at least a great deal of it).
%
We illustrate this by modifying the system under test (using bogus, non-logic-modifing modificaitons) and counting how many system and carved
tests are executed after each modification. In particular, we show that modification of few classes already caused all system tests to be triggered,
while only a relevant subset of carved tests are re-executed.
%
\xxx{For the sake of completeness we show how execution time varies during regression, compared to fully re-executing all the carved tests?}

Delta debugging -> this one becomes part of the evaluation. Since we adopt a conservative approach to include all the calls which might be state chaging,
we are interesting to investigate if this is "a bad thing". How many of those actions are superfluous and can be safely removed?
This also gives us a hint on if/how it would be beneficial to use delta debugging to minimize the carved tests cases.

For doing so, we implemente a delta debugging appraoch to remove and test each action, and compare the results on execution time, coverage and regression test selection of original and minimized test to see.
	generate validation instruction based on XML serialized state. We do not literally compare the serialized state, because some objects cannot be serialized in a meaningful way (i.e., open data base connections, file descriptors, streams) and some objects are parameters of the tests and are injected as dependencies during testing. For example, temporary working dir, date objects relative to current date, and other data that are generated randomly at the beginning of each test (and never changed during the execution?). 
	
	\xxx{Give examples here on Employee and HotelReservation ? this means we need to introduce them as motivating scenarios?}
	
	Otherwise, we cannot remove dependencies correctly. \xxx{This requires an example}.
	We compare both the return value of the MUT if any, and the state of CUT.
	
	Starting from the instruction before MUT and going backwards, we remove one instruction, repeat the test execution, and check that the test passes. That is, no unhandled exceptions and no failed assertions.

	We assume a reset environment function, i.e., wipe out the database, is provided by the user to avoid polluting, and invalidating the result of delta debugging. Note that the carved tests instead do not require this, and are correct by design.
	
	\xxx{How much do we save in terms of instructions? and execution time?}

This is simple but effective solutions, however, it incurrs in a huge performance overhead because each carved test must be re-executed a number of times equals to the amount of instructions it contains. Optimization based on static analysis and data-flow analysis might be used here to identify cluster of invocations that belong together and possibly can be removed or left in the carved test all together. However, this is out of the scope of this paper and will be addressed in future work.
	\xxx{How much does it cost to run delta debugging?}

Benefits of Delta Debugging. Since delta debugging is expensive, we want to investigate/assess its cost/benefits. We do so by comparing execution time and number of instructions before and after minimization, as well as regression test selection.
%
Note that in this cases the evaluation is biased by the choice of the test subject, which do not involve external services (which might be slow and expensive to use), but only locally available resources.

\xxx{Maybe seel this thing of delta debugging as part of the evaluation to show that, because delta debugging does not remove many instructions, 
then the carved tests are already good, cannot be improved in that sense. Note that this accounts for the instructions not the method invocations executed !}

Overhead of carving.
We are also interested in assessing the overhead of the overall approach to derive conclusions on its practical applicability.
For this, we measure and report execution and slow down factors of instrumentation, tracing, carving, test generation, and delta debugging.

I suspect that delta debugging might shows benefits for those classes which do not involve or depend on external interfaces,
and are used frequently.
For example HotelView has barely no logic, but to test it there's a lot of setup that can be removed.
Really relevant?


\xxx{Carved tests are faster to execute than system tests.}
\xxx{Maybe we can show this per class under test: we need to execute only a subset of tests}
\xxx{With our test subjects, we cannot really shows this, since the time}

\subsection{Test Subjects}

Employee project.
Interactive command line utility to manage employee and salaries. It works by creating, reading, and deleting files.
\xxx{check if the same (or similar) to original carving work}
\xxx{Add stats?}

HotelReservation System project.
Interactive command line utility to handle an hotel reservation system. It uses mysql backend to manage the state of the applications.
\xxx{Add stats?}

\subsection{Experimental Settings}
System tests. The test subjects do not come with system tests, so I created them manually.
System tests work by resetting the state of the system before each test, mocking input commands, and cleaning up the state of the system
to avoid pollution~\cite{pollution}.
Additionally, system tests set up expectations on System.exit to avoid system and carved tests to abruptly halting the execution.

For the  Employee projects, I rely on temporary folders and predefined file contents.
For the HotelReservation, I rely on truncating the database and recreate it as expected (either empty, or with predefined content).

\xxx{How system tests are defined? Ideally I would like to create them by following a standard technique, in practice, I will motivate my choice
by saying I creating them to cover as much application state as possible, in a black box style.
That is, I consider "each" page of the application as state and navigate the application states by means of different input sequences.
The name should be coverage-transition? Check on the software testing class}

% Maybe a bit to verbose
To answer RQ1 and RQ2, I execute the system tests and measure the achieved coverage;
Next, I instrument the application,  repeat the execution of the same system tests to collect the trace, 
and from the trace carve the tests out. I generate carved tests and minimized carved tests using delta debugging.
Finally, I execute the carved and minimized carved tests to measure coverage, time to carve tests, 
time to generate tests, and time to minimize test via delta debugging.

To answer RQ3 I proceed as follow. 
Usefulness of Carve Tests:
I execute the Ekstazi tool~\cite{} on the original version of the code separately for system and carved tests.
Then, I randomly change a number of source files n (n={1, \ldots, N}, N is the total amount of application files (no tests, no Main)) in the project.
Next, I re-execute Ekstazi separately for ssytem and carved tests, but keeping the same modification , and collect information on how many system and carved tests where executed upon each modification.
Ekstazi will select only the tests which are needed to be re-execute to test the modification.
The modification is bogus, we do not modify for simplicity interfaces, but only implementation classes to avoid compilation problems.
We repeat the process for 20 times and for n from 1 to N (N is application classes)

\subsection{Results}

%\begin{table*}
%\centering
%\caption{Summary of Test Carving}
%\begin{tabular}{l l l l l l l l l}
%Project & System Tests &  Carved Tests & Class Coverage & Statement Coverage & Branch Coverage & time SYS &  time Carv &  time Carv MIN\\
%Employee & 7  & 87 &  100\% & 35\% & 14\%  & 0.045 & 0.081 & --\\
%Hotel Reservation & 5 & 145 & 100\% & 41 \% & 29\% & 0.615 & 7.78 & --\\
%\end{tabular}
%\end{table*}

\begin{table*}
\centering
\caption{Summary of Test Carving.}
\begin{tabular}{l l
			r r r
			r r r
			r r r
			r r r 
			r}
\toprule
\multicolumn{2}{c}{Project} & 
	\multicolumn{3}{c}{Tests (\#)} &
	\multicolumn{3}{c}{Execution Time (sec)} &
	\multicolumn{3}{c}{Stmt. Coverage (\%)} &
	\multicolumn{3}{c}{Branch Coverage (\%)} &
	\multicolumn{1}{c}{Trace Size (\#)} \\
	\cmidrule(rl){1-2}
	\cmidrule(rl){3-5}
	\cmidrule(rl){6-8}
	\cmidrule(rl){9-11}
	\cmidrule(rl){12-14}
	
	
	\multicolumn{1}{c}{name} & \multicolumn{1}{c}{feature} &
	\multicolumn{1}{c}{\textbf{S}} & \multicolumn{1}{c}{\textbf{C}} & \multicolumn{1}{c}{\textbf{M}} & 
	\multicolumn{1}{c}{\textbf{S}} & \multicolumn{1}{c}{\textbf{C}} & \multicolumn{1}{c}{\textbf{M}} & 
	\multicolumn{1}{c}{\textbf{S}} & \multicolumn{1}{c}{\textbf{C}} & \multicolumn{1}{c}{\textbf{M}} &
	\multicolumn{1}{c}{\textbf{S}} & \multicolumn{1}{c}{\textbf{C}} & \multicolumn{1}{c}{\textbf{M}} &
	 \\
\midrule
Employee & File system &
	11  & 464 & 15 & 
	5.1 & 2.1 & 0.17 & 
	86\% & 86\% & 86\% &
	65\% & 65\% & 65\% &
	5470 \\
Hotel Reservation & SQL database & \\
\bottomrule
\end{tabular} \\
\flushleft{The table reports an overview of the results achieved by \abc.
The \textbf{S} label identifies results for system tests, while \textbf{C} and \textbf{M} identify respectively the results for the carved and minimized test suites.
}
%\xxx{20 repetitions for the execution time}s
%	\multicolumn{1}{c}{Carving Overhead} & \multicolumn{1}{c}{Reduction} & \multicolumn{1}{c}{Delta Debugging}
%& 4,106 
%\xxx{Repeat the analysis and that the average execution time}
\end{table*}

\begin{figure*}[t]
\centering
\begin{minipage}[b]{.45\textwidth}
\includegraphics[width=\columnwidth]{figures/employee-rts-size}
\caption{Test Regression Selection - Employee}
\xxx{Make a Box Plot here instead. Lines connecting the average values, but boxes to show min/max ? Maybe consider splitting the plot in differen plots one for each approach, so it might be easy to compare?}
\end{minipage}\hfill
\begin{minipage}[b]{.45\textwidth}
\includegraphics[width=\columnwidth]{figures/hotelme-rts-size}
\caption{Test Regression Selection - Hotel Reservation}
\end{minipage}
\end{figure*}


\begin{figure*}[t]
\centering
\begin{minipage}[b]{.45\textwidth}
\includegraphics[width=\columnwidth]{figures/employee-rts-time}
\caption{Test Regression Selection - Execution time - Employee}
\xxx{Not sure this is relevant - update the system tests to last longer and explore more app behaviors}
\end{minipage}\hfill
\begin{minipage}[b]{.45\textwidth}
\includegraphics[width=\columnwidth]{figures/hotelme-rts-time}
\caption{Test Regression Selection  - Execution time -  Hotel Reservation}
\xxx{Not sure this is relevant - update the system tests to last longer and explore more app behaviors}
\end{minipage}
\end{figure*}

\subsection{Observation}
\xxx{For employee, Why the number of carved tests keep changing for hotel me?}
\xxx{For some }


\subsection{Discussion}
Coverage is the same

Execution time is larger for carved unit tests fro projects with simple setup.

We can generate tests for all the classes

Does minimization make sense?


Should I keep or not the test setup calls intact? Should I instead disassemble those as well ?!

Delta Debugging dominates the test generation time.
There's no difference in RST with and without minimized test cases.
For SOME REASON the minimized tests it takes more time to execute than the normal ones..
%
How many tests minimized? and how much?
%Employee: 87, 13 minimized (~14\%), those have an avg minimization of 95.55\%

% Is there a differnce in RTS.
Employee: there no difference in RTS, exactly the same tests are selected upon the same modifications.

\subsection{Limitations}
Action based carving need to "see" how object instances are created, otherwise might not be able to perform the carving because there's no way for the carver to reconstruct the object instance.

We observed this problem for example with test setup calls?

\subsection{Discussion}

Complex programs with GUI and WebApplications call for a hybrid approach: the hybrid approach provides (if possible) the parameters to the entry points of the application, for example, the servlet or the logic behind the GUI, which otherwise cannot observe the object creation thus replicate it.
With an hybrid approach, one might serialize the parameters of the calls and reload them to spin off the carving. E.g., one can define mocks that provide http requests, fill in with values objserved during the execution.

\end{document}